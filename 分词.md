## 分词

### 介绍
> 分词是指将文本转换成一系列单词（term or token）的过程，也可以叫做文本分析，在 es 里面称为 Analysis

| 文本 | elasticsearch 是最流行的搜索搜索引擎 |
| --- | --- |
| 分词结果 | Elasticsearch 流行 搜索引擎 |

### 分词器
> 分词器是 es 中专门处理分词的组件，英文为 Analyzer

#### 组成部分

- Character Filters
    - 针对原始文本进行处理，比如去除 html 特殊标记符
- Tokenizer
    - 将原始文本按照一定规则切分为单词
- Token Filters
    - 针对 tokenize 处理的单词就行再加工，比如转小写、删除或新增等处理

#### 调用顺序
Character Filters -> Tokenizer -> Token Filters

### Analyze API
> es 提供一个测试分词的 api 接口，方便验证分词结果，endpoint 是 \_analyze

#### 可以直接指定 analyzer 进行测试
> 格式：POST /\_analyze http-body

```
# request
# analyzer 分词器
# text 测试文本
POST /_analyze
{
  "analyzer": "standard",
  "text": "hello world"
}

# response
# token 分词结果
# start_offset 起始偏移
# end_offset 结束偏移
# position 分词位置
{
  "tokens" : [
    {
      "token" : "hello",
      "start_offset" : 0,
      "end_offset" : 5,
      "type" : "<ALPHANUM>",
      "position" : 0
    },
    {
      "token" : "world",
      "start_offset" : 6,
      "end_offset" : 11,
      "type" : "<ALPHANUM>",
      "position" : 1
    }
  ]
}
```

#### 可以直接指定索引中的字段进行测试
> 格式：POST /index-name/\_analyze http-body

```
# request
# field 测试字段
# text 测试文本
POST /demo/_analyze
{
  "field": "username",
  "text": "hello world"
}

# response
{
  "tokens" : [
    {
      "token" : "hello",
      "start_offset" : 0,
      "end_offset" : 5,
      "type" : "<ALPHANUM>",
      "position" : 0
    },
    {
      "token" : "world",
      "start_offset" : 6,
      "end_offset" : 11,
      "type" : "<ALPHANUM>",
      "position" : 1
    }
  ]
}
```

#### 可以自定义分词器进行测试
> 格式：POST /\_analyze http-body

```
# request
POST /_analyze
{
  "tokenizer": "standard",
  "filter": ["lowercase"],
  "text": "Hello World"
}

# respoonse
{
  "tokens" : [
    {
      "token" : "hello",
      "start_offset" : 0,
      "end_offset" : 5,
      "type" : "<ALPHANUM>",
      "position" : 0
    },
    {
      "token" : "world",
      "start_offset" : 6,
      "end_offset" : 11,
      "type" : "<ALPHANUM>",
      "position" : 1
    }
  ]
}
```

### es 自带如下的分词器
#### standard
- 默认分词器
- 组成
    - Tokenizer
        - standard
    - filter
        - standard
        - lowercase
        - stop (disabled by default)
- 特性
    - 按词切分，支持多语言
    - 小写处理

```
# request
PSOT /_analyze
{
  "analyzer": "standard",
  "text": "The 2 QUICK Brown-Foxes jumped over the lazy dog's bone."
}

# response
{
  "tokens" : [
    {
      "token" : "the",
      "start_offset" : 0,
      "end_offset" : 3,
      "type" : "<ALPHANUM>",
      "position" : 0
    },
    {
      "token" : "2",
      "start_offset" : 4,
      "end_offset" : 5,
      "type" : "<NUM>",
      "position" : 1
    },
    {
      "token" : "quick",
      "start_offset" : 6,
      "end_offset" : 11,
      "type" : "<ALPHANUM>",
      "position" : 2
    },
    {
      "token" : "brown",
      "start_offset" : 12,
      "end_offset" : 17,
      "type" : "<ALPHANUM>",
      "position" : 3
    },
    {
      "token" : "foxes",
      "start_offset" : 18,
      "end_offset" : 23,
      "type" : "<ALPHANUM>",
      "position" : 4
    },
    {
      "token" : "jumped",
      "start_offset" : 24,
      "end_offset" : 30,
      "type" : "<ALPHANUM>",
      "position" : 5
    },
    {
      "token" : "over",
      "start_offset" : 31,
      "end_offset" : 35,
      "type" : "<ALPHANUM>",
      "position" : 6
    },
    {
      "token" : "the",
      "start_offset" : 36,
      "end_offset" : 39,
      "type" : "<ALPHANUM>",
      "position" : 7
    },
    {
      "token" : "lazy",
      "start_offset" : 40,
      "end_offset" : 44,
      "type" : "<ALPHANUM>",
      "position" : 8
    },
    {
      "token" : "dog's",
      "start_offset" : 45,
      "end_offset" : 50,
      "type" : "<ALPHANUM>",
      "position" : 9
    },
    {
      "token" : "bone",
      "start_offset" : 51,
      "end_offset" : 55,
      "type" : "<ALPHANUM>",
      "position" : 10
    }
  ]
}
```

#### simple
- 组成
    - Tokenizer
        - lowercase
- 特征
    - 按照非字母切分
    - 小写处理

```
# reqpuest
POST /_analyze
{
  "analyzer": "simple",
  "text": "The 2 QUICK Brown-Foxes jumped over the lazy dog's bone."
}

# response
{
  "tokens" : [
    {
      "token" : "the",
      "start_offset" : 0,
      "end_offset" : 3,
      "type" : "word",
      "position" : 0
    },
    {
      "token" : "quick",
      "start_offset" : 6,
      "end_offset" : 11,
      "type" : "word",
      "position" : 1
    },
    {
      "token" : "brown",
      "start_offset" : 12,
      "end_offset" : 17,
      "type" : "word",
      "position" : 2
    },
    {
      "token" : "foxes",
      "start_offset" : 18,
      "end_offset" : 23,
      "type" : "word",
      "position" : 3
    },
    {
      "token" : "jumped",
      "start_offset" : 24,
      "end_offset" : 30,
      "type" : "word",
      "position" : 4
    },
    {
      "token" : "over",
      "start_offset" : 31,
      "end_offset" : 35,
      "type" : "word",
      "position" : 5
    },
    {
      "token" : "the",
      "start_offset" : 36,
      "end_offset" : 39,
      "type" : "word",
      "position" : 6
    },
    {
      "token" : "lazy",
      "start_offset" : 40,
      "end_offset" : 44,
      "type" : "word",
      "position" : 7
    },
    {
      "token" : "dog",
      "start_offset" : 45,
      "end_offset" : 48,
      "type" : "word",
      "position" : 8
    },
    {
      "token" : "s",
      "start_offset" : 49,
      "end_offset" : 50,
      "type" : "word",
      "position" : 9
    },
    {
      "token" : "bone",
      "start_offset" : 51,
      "end_offset" : 55,
      "type" : "word",
      "position" : 10
    }
  ]
}
```

#### whitespace
- 组成
    - Tokenizer
        - whitespace
- 特征
    - 按照空额切分

```
# request
POST /_analyze
{
  "analyzer": "whitespace",
  "text": "The 2 QUICK Brown-Foxes jumped over the lazy dog's bone."
}

# response
{
  "tokens" : [
    {
      "token" : "The",
      "start_offset" : 0,
      "end_offset" : 3,
      "type" : "word",
      "position" : 0
    },
    {
      "token" : "2",
      "start_offset" : 4,
      "end_offset" : 5,
      "type" : "word",
      "position" : 1
    },
    {
      "token" : "QUICK",
      "start_offset" : 6,
      "end_offset" : 11,
      "type" : "word",
      "position" : 2
    },
    {
      "token" : "Brown-Foxes",
      "start_offset" : 12,
      "end_offset" : 23,
      "type" : "word",
      "position" : 3
    },
    {
      "token" : "jumped",
      "start_offset" : 24,
      "end_offset" : 30,
      "type" : "word",
      "position" : 4
    },
    {
      "token" : "over",
      "start_offset" : 31,
      "end_offset" : 35,
      "type" : "word",
      "position" : 5
    },
    {
      "token" : "the",
      "start_offset" : 36,
      "end_offset" : 39,
      "type" : "word",
      "position" : 6
    },
    {
      "token" : "lazy",
      "start_offset" : 40,
      "end_offset" : 44,
      "type" : "word",
      "position" : 7
    },
    {
      "token" : "dog's",
      "start_offset" : 45,
      "end_offset" : 50,
      "type" : "word",
      "position" : 8
    },
    {
      "token" : "bone.",
      "start_offset" : 51,
      "end_offset" : 56,
      "type" : "word",
      "position" : 9
    }
  ]
}
```

#### stop
- Stop Word 指语气助词等修饰性的词语，比如 the、an、的、这等等
- 组成
    - Tokenizer
        - lowercase
    - filters
        - stop
- 特征
    - 相比 Simple Analyzer 多了 Stop Word 处理

```
# request
POST /_analyze
{
  "analyzer": "stop",
  "text": "The 2 QUICK Brown-Foxes jumped over the lazy dog's bone."
}

# response
{
  "tokens" : [
    {
      "token" : "quick",
      "start_offset" : 6,
      "end_offset" : 11,
      "type" : "word",
      "position" : 1
    },
    {
      "token" : "brown",
      "start_offset" : 12,
      "end_offset" : 17,
      "type" : "word",
      "position" : 2
    },
    {
      "token" : "foxes",
      "start_offset" : 18,
      "end_offset" : 23,
      "type" : "word",
      "position" : 3
    },
    {
      "token" : "jumped",
      "start_offset" : 24,
      "end_offset" : 30,
      "type" : "word",
      "position" : 4
    },
    {
      "token" : "over",
      "start_offset" : 31,
      "end_offset" : 35,
      "type" : "word",
      "position" : 5
    },
    {
      "token" : "lazy",
      "start_offset" : 40,
      "end_offset" : 44,
      "type" : "word",
      "position" : 7
    },
    {
      "token" : "dog",
      "start_offset" : 45,
      "end_offset" : 48,
      "type" : "word",
      "position" : 8
    },
    {
      "token" : "s",
      "start_offset" : 49,
      "end_offset" : 50,
      "type" : "word",
      "position" : 9
    },
    {
      "token" : "bone",
      "start_offset" : 51,
      "end_offset" : 55,
      "type" : "word",
      "position" : 10
    }
  ]
}
```

#### keyword
- 组成
    - Tokenizer
        - keyword
- 特征
    - 不分词，直接将输入作为一个单词输出

```
# reqpuset
{
  "analyzer": "keyword",
  "text": "The 2 QUICK Brown-Foxes jumped over the lazy dog's bone."
}

# response
{
  "tokens" : [
    {
      "token" : "The 2 QUICK Brown-Foxes jumped over the lazy dog's bone.",
      "start_offset" : 0,
      "end_offset" : 56,
      "type" : "word",
      "position" : 0
    }
  ]
}
```

#### pattern
- 组成
    - Tokenizer
        - pattern
    - Filters
        - lowercase
        - stop(disanled by default)
- 特征
    - 通过正则表达式自定义分隔符
    - 默认是 \W+，即非字词的符号作为分隔符

```
# request
POST /_analyze
{
  "analyzer": "pattern",
  "text": "The 2 QUICK Brown-Foxes jumped over the lazy dog's bone."
}

# response
{
  "tokens" : [
    {
      "token" : "the",
      "start_offset" : 0,
      "end_offset" : 3,
      "type" : "word",
      "position" : 0
    },
    {
      "token" : "2",
      "start_offset" : 4,
      "end_offset" : 5,
      "type" : "word",
      "position" : 1
    },
    {
      "token" : "quick",
      "start_offset" : 6,
      "end_offset" : 11,
      "type" : "word",
      "position" : 2
    },
    {
      "token" : "brown",
      "start_offset" : 12,
      "end_offset" : 17,
      "type" : "word",
      "position" : 3
    },
    {
      "token" : "foxes",
      "start_offset" : 18,
      "end_offset" : 23,
      "type" : "word",
      "position" : 4
    },
    {
      "token" : "jumped",
      "start_offset" : 24,
      "end_offset" : 30,
      "type" : "word",
      "position" : 5
    },
    {
      "token" : "over",
      "start_offset" : 31,
      "end_offset" : 35,
      "type" : "word",
      "position" : 6
    },
    {
      "token" : "the",
      "start_offset" : 36,
      "end_offset" : 39,
      "type" : "word",
      "position" : 7
    },
    {
      "token" : "lazy",
      "start_offset" : 40,
      "end_offset" : 44,
      "type" : "word",
      "position" : 8
    },
    {
      "token" : "dog",
      "start_offset" : 45,
      "end_offset" : 48,
      "type" : "word",
      "position" : 9
    },
    {
      "token" : "s",
      "start_offset" : 49,
      "end_offset" : 50,
      "type" : "word",
      "position" : 10
    },
    {
      "token" : "bone",
      "start_offset" : 51,
      "end_offset" : 55,
      "type" : "word",
      "position" : 11
    }
  ]
}
```

#### language
 - 提供了 30+ 常见语言的分词器
    - arabic, armenianm, basquem, bengali, brazilian, bulgarian, catalan, cjk, czech, danish, duth, english...

### 中文分词
#### 难点
- 中文分词指的是将一个汉字序列切分成一个一个单独的词，在英文中，单词之间是以空格作为自然分界符，汉语中词没有一个形式上的分界符
- 上下文不同，分词结果迥异，你如交叉歧义问题，比如下面两种分词都合理
    - 乒乓球拍/卖/完了
    - 乒乓球/拍卖/完了

#### 常用分词系统
- IK 
    - 实现中英文单词的切分，支持 ik\_smart、ik\_maxword 等模式
    - 可自动以词库，支持热更新分词词典
- jjeba
    - python 中最流行的分词系统，支持分词和词性标注
    - 支持繁体分词、自定义词典、并行分词等

#### 基于自然语言处理的分词系统
- Hanlp
    - 由一系列模型与算法组成的 Java 工具包，目标是普及自然语言处理在生产环境中的应用
- THULAC
    - THU Lexical Analyzer for Chinese，由清华大学自然语言处理于社会人文计算实验室研制退出的一套中文词法分析工具包，具有中文分词和词性标注功能

### 自定义分词
#### Character Filters
= 在 Tokenizer 之前对原始文本进行处理，比如增加、删减或替换字符等
- 自带
    - HTML Strip 去除 html 标签和转换 html 实体
    - Mapping 进行字符替换操作
    - Pattern Replace 进行正则匹配替换
- 会影响后续 Tokenizer 解析的 postion 和 offset 信息

```
# request
# char_filter 指明使用的 char_filter
POST /_analyze
{
  "tokenizer": "keyword",
  "char_filter": ["html_strip"],
  "text": "<p>I&apos;m so <b>happy</b>!</p>"
}

# response
{
  "tokens" : [
    {
      "token" : """

I'm so happy!

""",
      "start_offset" : 0,
      "end_offset" : 32,
      "type" : "word",
      "position" : 0
    }
  ]
}
```

#### Tokenize
- 将原始文本按照一定规则切分为单词（term or token）
- 自带
    - standard 按照单词进行分割
    - letter 按照非字符类进行分割
    - whitespace 按照空格进行分割
    - UAC URL Email 按照 standard 分割，但不会分割邮箱和 url
    - NGram 和 Edge NGram 连词分割
    - Path Hierarchy 按照文件路劲进行分割

```
# request
POST /_analyze
{
  "tokenizer": "path_hierarchy",
  "text": "/one/two/three"
}

# response
{
  "tokens" : [
    {
      "token" : "/one",
      "start_offset" : 0,
      "end_offset" : 4,
      "type" : "word",
      "position" : 0
    },
    {
      "token" : "/one/two",
      "start_offset" : 0,
      "end_offset" : 8,
      "type" : "word",
      "position" : 0
    },
    {
      "token" : "/one/two/three",
      "start_offset" : 0,
      "end_offset" : 14,
      "type" : "word",
      "position" : 0
    }
  ]
}
```

#### Token Filters
- 对于 tokenizer 输出的单词（term）进行增加、删除、修改等操作
- 自带
    - lowercase 将所有 term 转换为小写
    - stop 删除 stop words
    - NGram 和 Edge NGram 连词分割
    - Synonym 添加近义词的 term

```
# request
POST /_analyze
{
  "tokenizer": "standard",
  "filter": [
    "stop",
    "lowercase",
    {
      "type": "ngram",
      "min_gran": 4,
      "max_gran": 4
    }
  ],
  "text": "a Hello,world!"
}

# response
{
  "tokens" : [
    {
      "token" : "hell",
      "start_offset" : 2,
      "end_offset" : 7,
      "type" : "<ALPHANUM>",
      "position" : 1
    },
    {
      "token" : "ello",
      "start_offset" : 2,
      "end_offset" : 7,
      "type" : "<ALPHANUM>",
      "position" : 1
    },
    {
      "token" : "worl",
      "start_offset" : 8,
      "end_offset" : 13,
      "type" : "<ALPHANUM>",
      "position" : 2
    },
    {
      "token" : "orld",
      "start_offset" : 8,
      "end_offset" : 13,
      "type" : "<ALPHANUM>",
      "position" : 2
    }
  ]
}
```

### 自定义分词 API
> 自定义分词需要在索引的配置中设定  
> 格式：PUT /index-name http-body

```
# reqpuest
PUT /demo 
{
    "setting": {
        "analysis": {
            "char_filter": {},
            "tokenizer": {},
            "filter": {},
            "analyzer": {}
        }
    }
}
```

#### 简单定义分词器，如下
- Character Filters
    - HTML_Strip
- Tokenizer
    - standard
- Token Filters
    - lowercase
    - ASCII Folding

```
# request
PUT /demo
{
  "settings": {
    "analysis": {
      "analyzer": {
        "my_custom_analyzer": {
          "type": "custom",
          "tokenizer": "standard",
          "char_filter": [
            "html_strip"
          ],
          "filter": [
            "lowercase",
            "asciifolding"
          ]
        }
      }
    }
  }
}

# response
{
  "acknowledged" : true,
  "shards_acknowledged" : true,
  "index" : "demo"
}

# 验证
# request
POST /demo/_analyze
{
  "analyzer": "my_custom_analyzer",
  "text": "Is this <b>a box</b>?"
}

# response
{
  "tokens" : [
    {
      "token" : "is",
      "start_offset" : 0,
      "end_offset" : 2,
      "type" : "<ALPHANUM>",
      "position" : 0
    },
    {
      "token" : "this",
      "start_offset" : 3,
      "end_offset" : 7,
      "type" : "<ALPHANUM>",
      "position" : 1
    },
    {
      "token" : "a",
      "start_offset" : 11,
      "end_offset" : 12,
      "type" : "<ALPHANUM>",
      "position" : 2
    },
    {
      "token" : "box",
      "start_offset" : 13,
      "end_offset" : 20,
      "type" : "<ALPHANUM>",
      "position" : 3
    }
  ]
}
```

#### 稍微复杂的定义分词器，如下
- Character Filters
    - custom mappingp
- Tokenizer
    - custom pattern
- Token Filters
    - lowercase
    - custom stop

```
# request
PUT /demo
{
  "settings": {
    "analysis": {
      "analyzer": {
        "my_custom_analyzer": {
          "type": "custom",
          "char_filter": [
            "my_custom_char_filter"
          ],
          "tokenizer": "my_custom_tokenizer",
          "filter": [
            "lowercase",
            "my_custom_filter"
          ]
        }
      },
      "tokenizer": {
        "my_custom_tokenizer": {
          "type": "pattern",
          "pattern": "[ .,!?]"
        }
      },
      "char_filter": {
        "my_custom_char_filter": {
          "type": "mapping",
          "mappings": [
            ":) => _happy_",
            ":( => _sad_"
          ]
        }
      },
      "filter": {
        "my_custom_filter": {
          "type": "stop",
          "stopwords": "_english_"
        }
      }
    }
  }
}

# response
{
  "acknowledged" : true,
  "shards_acknowledged" : true,
  "index" : "demo"
}

# 验证
# request
{
  "tokens" : [
    {
      "token" : "i'm",
      "start_offset" : 0,
      "end_offset" : 3,
      "type" : "word",
      "position" : 0
    },
    {
      "token" : "_happy_",
      "start_offset" : 6,
      "end_offset" : 8,
      "type" : "word",
      "position" : 2
    },
    {
      "token" : "person",
      "start_offset" : 9,
      "end_offset" : 15,
      "type" : "word",
      "position" : 3
    },
    {
      "token" : "you",
      "start_offset" : 21,
      "end_offset" : 24,
      "type" : "word",
      "position" : 5
    }
  ]
}
```
